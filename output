not adding op ResourceApplyPowerSign because Unsupported data type DT_RESOURCE
not adding op ResourceSparseApplyCenteredRMSProp because Unsupported data type DT_RESOURCE
not adding op ResourceSparseApplyRMSProp because Unsupported data type DT_RESOURCE
not adding op ResourceApplyRMSProp because Unsupported data type DT_RESOURCE
not adding op ResourceApplyAdamWithAmsgrad because Unsupported data type DT_RESOURCE
not adding op ResourceSparseApplyKerasMomentum because Unsupported data type DT_RESOURCE
not adding op ResourceApplyMomentum because Unsupported data type DT_RESOURCE
not adding op ResourceSparseApplyFtrlV2 because Unsupported data type DT_RESOURCE
not adding op ResourceSparseApplyFtrl because Unsupported data type DT_RESOURCE
not adding op ResourceApplyFtrl because Unsupported data type DT_RESOURCE
not adding op ResourceSparseApplyProximalAdagrad because Unsupported data type DT_RESOURCE
not adding op ResourceSparseApplyAdagradDA because Unsupported data type DT_RESOURCE
not adding op ResourceApplyAdagradDA because Unsupported data type DT_RESOURCE
not adding op ResourceApplyProximalAdagrad because Unsupported data type DT_RESOURCE
not adding op ResourceApplyAdagrad because Unsupported data type DT_RESOURCE
not adding op ResourceApplyProximalGradientDescent because Unsupported data type DT_RESOURCE
not adding op StringFormat because Type list not currently supported
not adding op ResourceSparseApplyProximalGradientDescent because Unsupported data type DT_RESOURCE
not adding op ResourceScatterNdAdd because Unsupported data type DT_RESOURCE
not adding op ResourceScatterNdUpdate because Unsupported data type DT_RESOURCE
not adding op ResourceSparseApplyAdagrad because Unsupported data type DT_RESOURCE
not adding op WriteGraphSummary because Unsupported data type DT_RESOURCE
not adding op WriteImageSummary because Unsupported data type DT_RESOURCE
not adding op ImportEvent because Unsupported data type DT_RESOURCE
not adding op WriteSummary because Unsupported data type DT_RESOURCE
not adding op CreateSummaryDbWriter because Unsupported data type DT_RESOURCE
not adding op CreateSummaryFileWriter because Unsupported data type DT_RESOURCE
not adding op SparseCross because Type list not currently supported
not adding op PyFunc because Type list not currently supported
not adding op ConsumeMutexLock because Unsupported data type DT_VARIANT
not adding op MutexLock because Unsupported data type DT_RESOURCE
not adding op MutexV2 because Unsupported data type DT_RESOURCE
not adding op ResourceScatterUpdate because Unsupported data type DT_RESOURCE
not adding op ResourceScatterMin because Unsupported data type DT_RESOURCE
not adding op ResourceScatterDiv because Unsupported data type DT_RESOURCE
not adding op ResourceScatterMul because Unsupported data type DT_RESOURCE
not adding op ResourceScatterSub because Unsupported data type DT_RESOURCE
not adding op ResourceScatterAdd because Unsupported data type DT_RESOURCE
not adding op ResourceGather because Unsupported data type DT_RESOURCE
not adding op VariableShape because Unsupported data type DT_RESOURCE
not adding op AssignSubVariableOp because Unsupported data type DT_RESOURCE
not adding op AssignAddVariableOp because Unsupported data type DT_RESOURCE
not adding op AssignVariableOp because Unsupported data type DT_RESOURCE
not adding op _VarHandlesOp because Unsupported data type DT_RESOURCE
not adding op RemoteFusedGraphExecute because Type list not currently supported
not adding op ResourceApplyFtrlV2 because Unsupported data type DT_RESOURCE
not adding op ParseExample because Type list not currently supported
not adding op ResourceApplyAddSign because Unsupported data type DT_RESOURCE
not adding op PyFuncStateless because Type list not currently supported
not adding op FilterDataset because Attr type func not supported
not adding op DeserializeIterator because Unsupported data type DT_RESOURCE
not adding op PrefetchDataset because Unsupported data type DT_VARIANT
not adding op RepeatDataset because Unsupported data type DT_VARIANT
not adding op MutableDenseHashTableV2 because Unsupported data type DT_RESOURCE
not adding op DecodeCSV because Type list not currently supported
not adding op MutableHashTableOfTensorsV2 because Unsupported data type DT_RESOURCE
not adding op MapPeek because Type list not currently supported
not adding op StagePeek because Type list not currently supported
not adding op ResourceApplyAdam because Unsupported data type DT_RESOURCE
not adding op GetSessionHandleV2 because Unsupported data type DT_RESOURCE
not adding op ReadVariableOp because Unsupported data type DT_RESOURCE
not adding op BarrierTakeMany because Type list not currently supported
not adding op ParseSequenceExample because Type list not currently supported
not adding op _XlaSendFromHost because Type list not currently supported
not adding op ResourceSparseApplyAdadelta because Unsupported data type DT_RESOURCE
not adding op TensorArraySizeV3 because Unsupported data type DT_RESOURCE
not adding op TensorArrayScatterV3 because Unsupported data type DT_RESOURCE
not adding op InfeedDequeueTuple because Type list not currently supported
not adding op TensorArrayGradWithShape because Unsupported data type DT_RESOURCE
not adding op CloseSummaryWriter because Unsupported data type DT_RESOURCE
not adding op StackV2 because Unsupported data type DT_RESOURCE
not adding op PaddingFIFOQueueV2 because Unsupported data type DT_RESOURCE
not adding op ExperimentalDenseToSparseBatchDataset because Unsupported data type DT_VARIANT
not adding op StackPopV2 because Unsupported data type DT_RESOURCE
not adding op StatefulPartitionedCall because Attr type func not supported
not adding op OptionalHasValue because Unsupported data type DT_VARIANT
not adding op QueueDequeueManyV2 because Unsupported data type DT_RESOURCE
not adding op TensorListFromTensor because Unsupported data type DT_VARIANT
not adding op IsBoostedTreesQuantileStreamResourceInitialized because Unsupported data type DT_RESOURCE
not adding op TensorArraySplitV3 because Unsupported data type DT_RESOURCE
not adding op ParallelInterleaveDatasetV2 because Attr type func not supported
not adding op QueueSizeV2 because Unsupported data type DT_RESOURCE
not adding op TensorForestTreePredict because Unsupported data type DT_RESOURCE
not adding op TensorForestTreeSize because Unsupported data type DT_RESOURCE
not adding op Batch because Type list not currently supported
not adding op SparseTensorSliceDataset because Unsupported data type DT_VARIANT
not adding op TensorArrayCloseV3 because Unsupported data type DT_RESOURCE
not adding op QueueDequeue because Type list not currently supported
not adding op TensorForestCreateTreeVariable because Unsupported data type DT_RESOURCE
not adding op TensorSliceDataset because Type list not currently supported
not adding op MapStage because Type list not currently supported
not adding op BoostedTreesQuantileStreamResourceHandleOp because Unsupported data type DT_RESOURCE
not adding op BoostedTreesSerializeEnsemble because Unsupported data type DT_RESOURCE
not adding op OptimizeDataset because Unsupported data type DT_VARIANT
not adding op ParseSingleExample because Type list not currently supported
not adding op WholeFileReaderV2 because Unsupported data type DT_RESOURCE
not adding op BoostedTreesDeserializeEnsemble because Unsupported data type DT_RESOURCE
not adding op IsBoostedTreesEnsembleInitialized because Unsupported data type DT_RESOURCE
not adding op Assert because Type list not currently supported
not adding op ParseSingleSequenceExample because Type list not currently supported
not adding op ExperimentalIgnoreErrorsDataset because Unsupported data type DT_VARIANT
not adding op FlatMapDataset because Attr type func not supported
not adding op TensorArrayGradV3 because Unsupported data type DT_RESOURCE
not adding op EncodeProto because Type list not currently supported
not adding op BoostedTreesCenterBias because Unsupported data type DT_RESOURCE
not adding op QueueEnqueueV2 because Unsupported data type DT_RESOURCE
not adding op QueueDequeueUpTo because Type list not currently supported
not adding op XlaIf because Attr type func not supported
not adding op LookupTableImportV2 because Unsupported data type DT_RESOURCE
not adding op TensorArrayConcatV3 because Unsupported data type DT_RESOURCE
not adding op BoostedTreesCreateEnsemble because Unsupported data type DT_RESOURCE
not adding op IteratorGetNextSync because Unsupported data type DT_RESOURCE
not adding op RandomShuffleQueueV2 because Unsupported data type DT_RESOURCE
not adding op QueueEnqueue because Type list not currently supported
not adding op XlaReduceWindow because Attr type func not supported
not adding op XlaSelectAndScatter because Attr type func not supported
not adding op ExperimentalParallelInterleaveDataset because Attr type func not supported
not adding op ParallelMapDataset because Attr type func not supported
not adding op TensorArrayGatherV3 because Unsupported data type DT_RESOURCE
not adding op ReaderResetV2 because Unsupported data type DT_RESOURCE
not adding op TakeDataset because Unsupported data type DT_VARIANT
not adding op TPUReplicate because Attr type func not supported
not adding op OrderedMapPeek because Type list not currently supported
not adding op XlaWhile because Attr type func not supported
not adding op BoostedTreesCreateQuantileStreamResource because Unsupported data type DT_RESOURCE
not adding op BoostedTreesQuantileStreamResourceFlush because Unsupported data type DT_RESOURCE
not adding op SymbolicGradient because Attr type func not supported
not adding op BatchDatasetV2 because Unsupported data type DT_VARIANT
not adding op For because Attr type func not supported
not adding op FixedLengthRecordReaderV2 because Unsupported data type DT_RESOURCE
not adding op BoostedTreesGetEnsembleStates because Unsupported data type DT_RESOURCE
not adding op ExperimentalAssertNextDataset because Unsupported data type DT_VARIANT
not adding op BoostedTreesExampleDebugOutputs because Unsupported data type DT_RESOURCE
not adding op ConcatenateDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalMaxIntraOpParallelismDataset because Unsupported data type DT_VARIANT
not adding op ResourceSparseApplyMomentum because Unsupported data type DT_RESOURCE
not adding op ExperimentalIndexedDatasetGet because Unsupported data type DT_RESOURCE
not adding op BoostedTreesEnsembleResourceHandleOp because Unsupported data type DT_RESOURCE
not adding op OutfeedEnqueueTuple because Type list not currently supported
not adding op _XlaRecvAtHost because Type list not currently supported
not adding op QueueIsClosedV2 because Unsupported data type DT_RESOURCE
not adding op TensorListGather because Unsupported data type DT_VARIANT
not adding op PriorityQueueV2 because Unsupported data type DT_RESOURCE
not adding op TensorListPushBack because Unsupported data type DT_VARIANT
not adding op QueueEnqueueManyV2 because Unsupported data type DT_RESOURCE
not adding op Unstage because Type list not currently supported
not adding op FakeQueue because Unsupported data type DT_RESOURCE
not adding op ResourceStridedSliceAssign because Unsupported data type DT_RESOURCE
not adding op MultiDeviceIterator because Unsupported data type DT_RESOURCE
not adding op BoostedTreesTrainingPredict because Unsupported data type DT_RESOURCE
not adding op EmptyTensorList because Unsupported data type DT_VARIANT
not adding op ResourceApplyAdaMax because Unsupported data type DT_RESOURCE
not adding op ResourceApplyAdadelta because Unsupported data type DT_RESOURCE
not adding op GeneratorDataset because Attr type func not supported
not adding op InfeedEnqueueTuple because Type list not currently supported
not adding op ZipDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalDatasetCardinality because Unsupported data type DT_VARIANT
not adding op ExperimentalSqlDataset because Unsupported data type DT_VARIANT
not adding op PaddedBatchDataset because Unsupported data type DT_VARIANT
not adding op OutfeedDequeueTuple because Type list not currently supported
not adding op BatchFunction because Attr type func not supported
not adding op TensorDataset because Type list not currently supported
not adding op _XlaRun because Type list not currently supported
not adding op ExperimentalUnbatchDataset because Unsupported data type DT_VARIANT
not adding op Stage because Type list not currently supported
not adding op TensorArrayWriteV3 because Unsupported data type DT_RESOURCE
not adding op FilterByLastComponentDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalThreadPoolHandle because Unsupported data type DT_RESOURCE
not adding op TensorForestTreeSerialize because Unsupported data type DT_RESOURCE
not adding op MapUnstage because Type list not currently supported
not adding op StackPushV2 because Unsupported data type DT_RESOURCE
not adding op ExperimentalBytesProducedStatsDataset because Unsupported data type DT_VARIANT
not adding op OrderedMapStage because Type list not currently supported
not adding op ResourceScatterMax because Unsupported data type DT_RESOURCE
not adding op TensorForestTreeDeserialize because Unsupported data type DT_RESOURCE
not adding op OptionalFromValue because Type list not currently supported
not adding op BoostedTreesPredict because Unsupported data type DT_RESOURCE
not adding op OrderedMapUnstage because Type list not currently supported
not adding op BoostedTreesQuantileStreamResourceGetBucketBoundaries because Unsupported data type DT_RESOURCE
not adding op FlushSummaryWriter because Unsupported data type DT_RESOURCE
not adding op ExperimentalDatasetToTFRecord because Unsupported data type DT_VARIANT
not adding op QueueCloseV2 because Unsupported data type DT_RESOURCE
not adding op IteratorFromStringHandle because Unsupported data type DT_RESOURCE
not adding op XlaReduce because Attr type func not supported
not adding op SerializeIterator because Unsupported data type DT_RESOURCE
not adding op EagerPyFunc because Type list not currently supported
not adding op MapDataset because Attr type func not supported
not adding op _While because Attr type func not supported
not adding op IteratorToStringHandle because Unsupported data type DT_RESOURCE
not adding op WindowDataset because Unsupported data type DT_VARIANT
not adding op VarIsInitializedOp because Unsupported data type DT_RESOURCE
not adding op ReaderNumWorkUnitsCompletedV2 because Unsupported data type DT_RESOURCE
not adding op CacheDataset because Unsupported data type DT_VARIANT
not adding op FixedLengthRecordDataset because Unsupported data type DT_VARIANT
not adding op ResourceApplyCenteredRMSProp because Unsupported data type DT_RESOURCE
not adding op IdentityN because Type list not currently supported
not adding op ResourceCountUpTo because Unsupported data type DT_RESOURCE
not adding op StackCloseV2 because Unsupported data type DT_RESOURCE
not adding op QueueDequeueV2 because Unsupported data type DT_RESOURCE
not adding op InterleaveDataset because Attr type func not supported
not adding op BatchDataset because Unsupported data type DT_VARIANT
not adding op PaddedBatchDatasetV2 because Unsupported data type DT_VARIANT
not adding op SaveV2 because Type list not currently supported
not adding op XlaLaunch because Attr type func not supported
not adding op ShuffleDataset because Unsupported data type DT_VARIANT
not adding op TensorForestTreeResourceHandleOp because Unsupported data type DT_RESOURCE
not adding op ShuffleAndRepeatDataset because Unsupported data type DT_VARIANT
not adding op TextLineDataset because Unsupported data type DT_VARIANT
not adding op ReaderNumRecordsProducedV2 because Unsupported data type DT_RESOURCE
not adding op FixedLengthRecordDatasetV2 because Unsupported data type DT_VARIANT
not adding op Iterator because Unsupported data type DT_RESOURCE
not adding op OneShotIterator because Attr type func not supported
not adding op TensorListReserve because Unsupported data type DT_VARIANT
not adding op _ReadVariablesOp because Unsupported data type DT_RESOURCE
not adding op DatasetToSingleElement because Unsupported data type DT_VARIANT
not adding op ReduceDataset because Attr type func not supported
not adding op IteratorFromStringHandleV2 because Unsupported data type DT_RESOURCE
not adding op OptionalGetValue because Unsupported data type DT_VARIANT
not adding op IteratorGetNextAsOptional because Unsupported data type DT_RESOURCE
not adding op ModelDataset because Unsupported data type DT_VARIANT
not adding op MapDefun because Attr type func not supported
not adding op MultiDeviceIteratorInit because Unsupported data type DT_VARIANT
not adding op TensorForestTreeIsInitializedOp because Unsupported data type DT_RESOURCE
not adding op MultiDeviceIteratorGetNextFromShard because Unsupported data type DT_RESOURCE
not adding op MultiDeviceIteratorToStringHandle because Unsupported data type DT_RESOURCE
not adding op OptionalNone because Unsupported data type DT_VARIANT
not adding op DecodeProtoV2 because Type list not currently supported
not adding op TensorArrayReadV3 because Unsupported data type DT_RESOURCE
not adding op ExperimentalCSVDataset because Type list not currently supported
not adding op ExperimentalDirectedInterleaveDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalGroupByReducerDataset because Attr type func not supported
not adding op ExperimentalGroupByWindowDataset because Attr type func not supported
not adding op ExperimentalLatencyStatsDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalSleepDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalMapAndBatchDataset because Attr type func not supported
not adding op ExperimentalMapDataset because Attr type func not supported
not adding op BoostedTreesQuantileStreamResourceAddSummaries because Unsupported data type DT_RESOURCE
not adding op ExperimentalMatchingFilesDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalNonSerializableDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalParseExampleDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalRandomDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalScanDataset because Attr type func not supported
not adding op ExperimentalSetStatsAggregatorDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalSlidingWindowDataset because Unsupported data type DT_VARIANT
not adding op AnonymousIterator because Unsupported data type DT_RESOURCE
not adding op ExperimentalStatsAggregatorHandle because Unsupported data type DT_RESOURCE
not adding op MakeIterator because Unsupported data type DT_VARIANT
not adding op ExperimentalStatsAggregatorSummary because Unsupported data type DT_RESOURCE
not adding op ResourceApplyKerasMomentum because Unsupported data type DT_RESOURCE
not adding op DatasetToGraph because Unsupported data type DT_VARIANT
not adding op WriteHistogramSummary because Unsupported data type DT_RESOURCE
not adding op ExperimentalUniqueDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalIteratorGetDevice because Unsupported data type DT_RESOURCE
not adding op ExperimentalPrivateThreadPoolDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalThreadPoolDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalNumaMapAndBatchDataset because Attr type func not supported
not adding op ExperimentalLMDBDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalIdentityIndexedDataset because Unsupported data type DT_VARIANT
not adding op ExperimentalMaterializedIndexDatasetHandle because Unsupported data type DT_RESOURCE
not adding op ExperimentalIndexedDatasetMaterialize because Unsupported data type DT_VARIANT
not adding op FIFOQueueV2 because Unsupported data type DT_RESOURCE
not adding op RestoreV2 because Type list not currently supported
not adding op TensorArrayV3 because Unsupported data type DT_RESOURCE
not adding op QueueDequeueUpToV2 because Unsupported data type DT_RESOURCE
not adding op _ListToArray because Type list not currently supported
not adding op RemoteCall because Attr type func not supported
not adding op _ArrayToList because Type list not currently supported
not adding op _If because Attr type func not supported
not adding op StatelessIf because Attr type func not supported
not adding op If because Attr type func not supported
not adding op While because Attr type func not supported
not adding op StatelessWhile because Attr type func not supported
not adding op _XlaCompile because Attr type func not supported
not adding op PartitionedCall because Attr type func not supported
not adding op WriteAudioSummary because Unsupported data type DT_RESOURCE
not adding op LookupTableSizeV2 because Unsupported data type DT_RESOURCE
not adding op TFRecordDataset because Unsupported data type DT_VARIANT
not adding op IteratorGetNext because Unsupported data type DT_RESOURCE
not adding op UnwrapDatasetVariant because Unsupported data type DT_VARIANT
not adding op SaveSlices because Type list not currently supported
not adding op TextLineReaderV2 because Unsupported data type DT_RESOURCE
not adding op TFRecordReaderV2 because Unsupported data type DT_RESOURCE
not adding op IdentityReaderV2 because Unsupported data type DT_RESOURCE
not adding op ResourceApplyGradientDescent because Unsupported data type DT_RESOURCE
not adding op ReaderReadV2 because Unsupported data type DT_RESOURCE
not adding op ReaderReadUpToV2 because Unsupported data type DT_RESOURCE
not adding op BoostedTreesUpdateEnsemble because Unsupported data type DT_RESOURCE
not adding op ReaderSerializeStateV2 because Unsupported data type DT_RESOURCE
not adding op ReaderRestoreStateV2 because Unsupported data type DT_RESOURCE
not adding op TensorListPushBackBatch because Unsupported data type DT_VARIANT
not adding op BoostedTreesQuantileStreamResourceDeserialize because Unsupported data type DT_RESOURCE
not adding op WriteScalarSummary because Unsupported data type DT_RESOURCE
not adding op TensorListLength because Unsupported data type DT_VARIANT
not adding op TensorListPopBack because Unsupported data type DT_VARIANT
not adding op TensorListStack because Unsupported data type DT_VARIANT
not adding op IteratorV2 because Unsupported data type DT_RESOURCE
not adding op TensorListConcat because Unsupported data type DT_VARIANT
not adding op TensorListSplit because Unsupported data type DT_VARIANT
not adding op TensorListElementShape because Unsupported data type DT_VARIANT
not adding op TensorListGetItem because Unsupported data type DT_VARIANT
not adding op TensorListSetItem because Unsupported data type DT_VARIANT
not adding op TensorListScatter because Unsupported data type DT_VARIANT
not adding op TensorListConcatLists because Unsupported data type DT_VARIANT
not adding op Print because Type list not currently supported
not adding op LookupTableFindV2 because Unsupported data type DT_RESOURCE
not adding op LookupTableInsertV2 because Unsupported data type DT_RESOURCE
not adding op LookupTableRemoveV2 because Unsupported data type DT_RESOURCE
not adding op LookupTableExportV2 because Unsupported data type DT_RESOURCE
not adding op MapUnstageNoKey because Type list not currently supported
not adding op HashTableV2 because Unsupported data type DT_RESOURCE
not adding op MultiDeviceIteratorFromStringHandle because Unsupported data type DT_RESOURCE
not adding op MutableHashTableV2 because Unsupported data type DT_RESOURCE
not adding op InitializeTableV2 because Unsupported data type DT_RESOURCE
not adding op SkipDataset because Unsupported data type DT_VARIANT
not adding op RangeDataset because Unsupported data type DT_VARIANT
not adding op QueueDequeueMany because Type list not currently supported
not adding op OrderedMapUnstageNoKey because Type list not currently supported
not adding op Save because Type list not currently supported
not adding op QueueEnqueueMany because Type list not currently supported
not adding op InitializeTableFromTextFileV2 because Unsupported data type DT_RESOURCE
not adding op WrapDatasetVariant because Unsupported data type DT_VARIANT
not adding op SummaryWriter because Unsupported data type DT_RESOURCE
not adding op DestroyResourceOp because Unsupported data type DT_RESOURCE
not adding op VarHandleOp because Unsupported data type DT_RESOURCE
f) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("LeakyRelu", self.op_name)
        new_op.add_input(&self.features)
        new_op.set_attr_float("alpha", alpha)
        new_op.set_attr_type("T", T::data_type())
        new_op.finish()
    }
}

struct Relu6Grad<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    id: usize,
    op_name: Option<String>,
    gradients: Tensor<T>,
    features: Tensor<T>,
}

impl<T> Relu6Grad<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn backprops(&self) -> Tensor<T> {
        Tensor::<T>::new(self, 0)
    }

    fn new(gradients: Tensor<T>, features: Tensor<T>) {
        Self {
            id: new_id(),
            op_name: None,
            gradients,
            features,
        }
    }
}

impl<T> Operation for Relu6Grad<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn get_id(&self) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("Relu6Grad", self.op_name)
        new_op.add_input(&self.gradients)
        new_op.add_input(&self.features)
        new_op.set_attr_type("T", T::data_type())
        new_op.finish()
    }
}

struct Relu6<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    id: usize,
    op_name: Option<String>,
    features: Tensor<T>,
}

impl<T> Relu6<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn activations(&self) -> Tensor<T> {
        Tensor::<T>::new(self, 0)
    }

    fn new(features: Tensor<T>) {
        Self {
            id: new_id(),
            op_name: None,
            features,
        }
    }
}

impl<T> Operation for Relu6<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn get_id(&self) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("Relu6", self.op_name)
        new_op.add_input(&self.features)
        new_op.set_attr_type("T", T::data_type())
        new_op.finish()
    }
}

struct ReluGrad<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    id: usize,
    op_name: Option<String>,
    gradients: Tensor<T>,
    features: Tensor<T>,
}

impl<T> ReluGrad<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn backprops(&self) -> Tensor<T> {
        Tensor::<T>::new(self, 0)
    }

    fn new(gradients: Tensor<T>, features: Tensor<T>) {
        Self {
            id: new_id(),
            op_name: None,
            gradients,
            features,
        }
    }
}

impl<T> Operation for ReluGrad<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn get_id(&self) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("ReluGrad", self.op_name)
        new_op.add_input(&self.gradients)
        new_op.add_input(&self.features)
        new_op.set_attr_type("T", T::data_type())
        new_op.finish()
    }
}

struct Relu<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_QINT8_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    id: usize,
    op_name: Option<String>,
    features: Tensor<T>,
}

impl<T> Relu<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_QINT8_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn activations(&self) -> Tensor<T> {
        Tensor::<T>::new(self, 0)
    }

    fn new(features: Tensor<T>) {
        Self {
            id: new_id(),
            op_name: None,
            features,
        }
    }
}

impl<T> Operation for Relu<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_QINT8_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn get_id(&self) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("Relu", self.op_name)
        new_op.add_input(&self.features)
        new_op.set_attr_type("T", T::data_type())
        new_op.finish()
    }
}

struct Dilation2DBackpropInput<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    id: usize,
    op_name: Option<String>,
    input: Tensor<T>,
    filter: Tensor<T>,
    out_backprop: Tensor<T>,
    strides: Vec<i64>,
    rates: Vec<i64>,
    padding: String,
}

impl<T> Dilation2DBackpropInput<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn in_backprop(&self) -> Tensor<T> {
        Tensor::<T>::new(self, 0)
    }

    fn new(input: Tensor<T>, filter: Tensor<T>, out_backprop: Tensor<T>, strides: Vec<i64>, rates: Vec<i64>, padding: String) {
        Self {
            id: new_id(),
            op_name: None,
            input,
            filter,
            out_backprop,
            strides,
            rates,
            padding,
        }
    }
}

impl<T> Operation for Dilation2DBackpropInput<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn get_id(&self) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("Dilation2DBackpropInput", self.op_name)
        new_op.add_input(&self.input)
        new_op.add_input(&self.filter)
        new_op.add_input(&self.out_backprop)
        new_op.set_attr_int_list("strides", &strides)
        new_op.set_attr_int_list("rates", &rates)
        new_op.set_attr_string("padding", &padding)
        new_op.set_attr_type("T", T::data_type())
        new_op.finish()
    }
}

struct MaxPoolGradGradWithArgmax<Targmax, T>
where Targmax: TensorType,
      Targmax: con_or_DT_INT32_or_DT_INT64,
      T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    id: usize,
    op_name: Option<String>,
    input: Tensor<T>,
    grad: Tensor<T>,
    argmax: Tensor<Targmax>,
    ksize: Vec<i64>,
    strides: Vec<i64>,
    padding: String,
}

impl<Targmax, T> MaxPoolGradGradWithArgmax<Targmax, T>
where Targmax: TensorType,
      Targmax: con_or_DT_INT32_or_DT_INT64,
      T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn output(&self) -> Tensor<T> {
        Tensor::<T>::new(self, 0)
    }

    fn new(input: Tensor<T>, grad: Tensor<T>, argmax: Tensor<Targmax>, ksize: Vec<i64>, strides: Vec<i64>, padding: String) {
        Self {
            id: new_id(),
            op_name: None,
            input,
            grad,
            argmax,
            ksize,
            strides,
            padding,
        }
    }
}

impl<Targmax, T> Operation for MaxPoolGradGradWithArgmax<Targmax, T>
where Targmax: TensorType,
      Targmax: con_or_DT_INT32_or_DT_INT64,
      T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn get_id(&self) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("MaxPoolGradGradWithArgmax", self.op_name)
        new_op.add_input(&self.input)
        new_op.add_input(&self.grad)
        new_op.add_input(&self.argmax)
        new_op.set_attr_int_list("ksize", &ksize)
        new_op.set_attr_int_list("strides", &strides)
        new_op.set_attr_string("padding", &padding)
        new_op.set_attr_type("Targmax", Targmax::data_type())
        new_op.set_attr_type("T", T::data_type())
        new_op.finish()
    }
}

struct MaxPoolGradWithArgmax<Targmax, T>
where Targmax: TensorType,
      Targmax: con_or_DT_INT32_or_DT_INT64,
      T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    id: usize,
    op_name: Option<String>,
    input: Tensor<T>,
    grad: Tensor<T>,
    argmax: Tensor<Targmax>,
    ksize: Vec<i64>,
    strides: Vec<i64>,
    padding: String,
}

impl<Targmax, T> MaxPoolGradWithArgmax<Targmax, T>
where Targmax: TensorType,
      Targmax: con_or_DT_INT32_or_DT_INT64,
      T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn output(&self) -> Tensor<T> {
        Tensor::<T>::new(self, 0)
    }

    fn new(input: Tensor<T>, grad: Tensor<T>, argmax: Tensor<Targmax>, ksize: Vec<i64>, strides: Vec<i64>, padding: String) {
        Self {
            id: new_id(),
            op_name: None,
            input,
            grad,
            argmax,
            ksize,
            strides,
            padding,
        }
    }
}

impl<Targmax, T> Operation for MaxPoolGradWithArgmax<Targmax, T>
where Targmax: TensorType,
      Targmax: con_or_DT_INT32_or_DT_INT64,
      T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn get_id(&self) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("MaxPoolGradWithArgmax", self.op_name)
        new_op.add_input(&self.input)
        new_op.add_input(&self.grad)
        new_op.add_input(&self.argmax)
        new_op.set_attr_int_list("ksize", &ksize)
        new_op.set_attr_int_list("strides", &strides)
        new_op.set_attr_string("padding", &padding)
        new_op.set_attr_type("Targmax", Targmax::data_type())
        new_op.set_attr_type("T", T::data_type())
        new_op.finish()
    }
}

struct ParseTensor<out_type>
where out_type: TensorType,
{
    id: usize,
    op_name: Option<String>,
    serialized: Tensor<String>,
}

impl<out_type> ParseTensor<out_type>
where out_type: TensorType,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn output(&self) -> Tensor<out_type> {
        Tensor::<out_type>::new(self, 0)
    }

    fn new(serialized: Tensor<String>) {
        Self {
            id: new_id(),
            op_name: None,
            serialized,
        }
    }
}

impl<out_type> Operation for ParseTensor<out_type>
where out_type: TensorType,
{
    fn get_id(&self) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("ParseTensor", self.op_name)
        new_op.add_input(&self.serialized)
        new_op.set_attr_type("out_type", out_type::data_type())
        new_op.finish()
    }
}

struct MaxPoolWithArgmax<Targmax, T>
where Targmax: TensorType,
      Targmax: con_or_DT_INT32_or_DT_INT64,
      T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    id: usize,
    op_name: Option<String>,
    input: Tensor<T>,
    ksize: Vec<i64>,
    strides: Vec<i64>,
    padding: String,
}

impl<Targmax, T> MaxPoolWithArgmax<Targmax, T>
where Targmax: TensorType,
      Targmax: con_or_DT_INT32_or_DT_INT64,
      T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn output(&self) -> Tensor<T> {
        Tensor::<T>::new(self, 0)
    }

    fn argmax(&self) -> Tensor<Targmax> {
        Tensor::<Targmax>::new(self, 1)
    }

    fn new(input: Tensor<T>, ksize: Vec<i64>, strides: Vec<i64>, padding: String) {
        Self {
            id: new_id(),
            op_name: None,
            input,
            ksize,
            strides,
            padding,
        }
    }
}

impl<Targmax, T> Operation for MaxPoolWithArgmax<Targmax, T>
where Targmax: TensorType,
      Targmax: con_or_DT_INT32_or_DT_INT64,
      T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn get_id(&self) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("MaxPoolWithArgmax", self.op_name)
        new_op.add_input(&self.input)
        new_op.set_attr_int_list("ksize", &ksize)
        new_op.set_attr_int_list("strides", &strides)
        new_op.set_attr_string("padding", &padding)
        new_op.set_attr_type("Targmax", Targmax::data_type())
        new_op.set_attr_type("T", T::data_type())
        new_op.finish()
    }
}

struct MaxPoolGradGradV2<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    id: usize,
    op_name: Option<String>,
    orig_input: Tensor<T>,
    orig_output: Tensor<T>,
    grad: Tensor<T>,
    ksize: Tensor<i32>,
    strides: Tensor<i32>,
    padding: String,
    data_format: String,
}

impl<T> MaxPoolGradGradV2<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn data_format(&mut self, data_format: String) -> &mut Self {
        self.data_format = data_format;
        self
    }

    fn output(&self) -> Tensor<T> {
        Tensor::<T>::new(self, 0)
    }

    fn new(orig_input: Tensor<T>, orig_output: Tensor<T>, grad: Tensor<T>, ksize: Tensor<i32>, strides: Tensor<i32>, padding: String) {
        Self {
            id: new_id(),
            op_name: None,
            orig_input,
            orig_output,
            grad,
            ksize,
            strides,
            padding,
            data_format: String::from_utf_lossy(&vec![78_u8, 72_u8, 87_u8, 67_u8,]),
        }
    }
}

impl<T> Operation for MaxPoolGradGradV2<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn get_id(&self) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("MaxPoolGradGradV2", self.op_name)
        new_op.add_input(&self.orig_input)
        new_op.add_input(&self.orig_output)
        new_op.add_input(&self.grad)
        new_op.add_input(&self.ksize)
        new_op.add_input(&self.strides)
        new_op.set_attr_string("padding", &padding)
        new_op.set_attr_string("data_format", &data_format)
        new_op.set_attr_type("T", T::data_type())
        new_op.finish()
    }
}

struct MaxPoolGrad<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    id: usize,
    op_name: Option<String>,
    orig_input: Tensor<T>,
    orig_output: Tensor<T>,
    grad: Tensor<T>,
    ksize: Vec<i64>,
    strides: Vec<i64>,
    padding: String,
    data_format: String,
}

impl<T> MaxPoolGrad<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn data_format(&mut self, data_format: String) -> &mut Self {
        self.data_format = data_format;
        self
    }

    fn output(&self) -> Tensor<T> {
        Tensor::<T>::new(self, 0)
    }

    fn new(orig_input: Tensor<T>, orig_output: Tensor<T>, grad: Tensor<T>, ksize: Vec<i64>, strides: Vec<i64>, padding: String) {
        Self {
            id: new_id(),
            op_name: None,
            orig_input,
            orig_output,
            grad,
            ksize,
            strides,
            padding,
            data_format: String::from_utf_lossy(&vec![78_u8, 72_u8, 87_u8, 67_u8,]),
        }
    }
}

impl<T> Operation for MaxPoolGrad<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF_or_DT_UINT32_or_DT_UINT64,
{
    fn get_id(&self) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("MaxPoolGrad", self.op_name)
        new_op.add_input(&self.orig_input)
        new_op.add_input(&self.orig_output)
        new_op.add_input(&self.grad)
        new_op.set_attr_int_list("ksize", &ksize)
        new_op.set_attr_int_list("strides", &strides)
        new_op.set_attr_string("padding", &padding)
        new_op.set_attr_string("data_format", &data_format)
        new_op.set_attr_type("T", T::data_type())
        new_op.finish()
    }
}

struct MaxPool<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_QINT8_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF,
{
    id: usize,
    op_name: Option<String>,
    input: Tensor<T>,
    ksize: Vec<i64>,
    strides: Vec<i64>,
    padding: String,
    data_format: String,
}

impl<T> MaxPool<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_QINT8_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn data_format(&mut self, data_format: String) -> &mut Self {
        self.data_format = data_format;
        self
    }

    fn output(&self) -> Tensor<T> {
        Tensor::<T>::new(self, 0)
    }

    fn new(input: Tensor<T>, ksize: Vec<i64>, strides: Vec<i64>, padding: String) {
        Self {
            id: new_id(),
            op_name: None,
            input,
            ksize,
            strides,
            padding,
            data_format: String::from_utf_lossy(&vec![78_u8, 72_u8, 87_u8, 67_u8,]),
        }
    }
}

impl<T> Operation for MaxPool<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_INT64_or_DT_QINT8_or_DT_BFLOAT16_or_DT_UINT16_or_DT_HALF,
{
    fn get_id(&self) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("MaxPool", self.op_name)
        new_op.add_input(&self.input)
        new_op.set_attr_int_list("ksize", &ksize)
        new_op.set_attr_int_list("strides", &strides)
        new_op.set_attr_string("padding", &padding)
        new_op.set_attr_string("data_format", &data_format)
        new_op.set_attr_type("T", T::data_type())
        new_op.finish()
    }
}

struct LRN<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_BFLOAT16_or_DT_HALF,
{
    id: usize,
    op_name: Option<String>,
    input: Tensor<T>,
    depth_radius: i64,
    bias: f32,
    alpha: f32,
    beta: f32,
}

impl<T> LRN<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_BFLOAT16_or_DT_HALF,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn depth_radius(&mut self, depth_radius: i64) -> &mut Self {
        self.depth_radius = depth_radius;
        self
    }

    fn bias(&mut self, bias: f32) -> &mut Self {
        self.bias = bias;
        self
    }

    fn alpha(&mut self, alpha: f32) -> &mut Self {
        self.alpha = alpha;
        self
    }

    fn beta(&mut self, beta: f32) -> &mut Self {
        self.beta = beta;
        self
    }

    fn output(&self) -> Tensor<T> {
        Tensor::<T>::new(self, 0)
    }

    fn new(input: Tensor<T>) {
        Self {
            id: new_id(),
            op_name: None,
            input,
            depth_radius: 5_i64,
            bias: 1_f32,
            alpha: 1_f32,
            beta: 0.5_f32,
        }
    }
}

impl<T> Operation for LRN<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_BFLOAT16_or_DT_HALF,
{
    fn get_id(&self) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("LRN", self.op_name)
        new_op.add_input(&self.input)
        new_op.set_attr_int("depth_radius", depth_radius)
        new_op.set_attr_float("bias", bias)
        new_op.set_attr_float("alpha", alpha)
        new_op.set_attr_float("beta", beta)
        new_op.set_attr_type("T", T::data_type())
        new_op.finish()
    }
}

struct DenseToSparseSetOperation<T>
where T: TensorType,
      T: con_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_STRING_or_DT_INT64_or_DT_UINT16,
{
    id: usize,
    op_name: Option<String>,
    set1: Tensor<T>,
    set2_indices: Tensor<i64>,
    set2_values: Tensor<T>,
    set2_shape: Tensor<i64>,
    set_operation: String,
    validate_indices: bool,
}

impl<T> DenseToSparseSetOperation<T>
where T: TensorType,
      T: con_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_STRING_or_DT_INT64_or_DT_UINT16,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn validate_indices(&mut self, validate_indices: bool) -> &mut Self {
        self.validate_indices = validate_indices;
        self
    }

    fn result_indices(&self) -> Tensor<i64> {
        Tensor::<i64>::new(self, 0)
    }

    fn result_values(&self) -> Tensor<T> {
        Tensor::<T>::new(self, 1)
    }

    fn result_shape(&self) -> Tensor<i64> {
        Tensor::<i64>::new(self, 2)
    }

    fn new(set1: Tensor<T>, set2_indices: Tensor<i64>, set2_values: Tensor<T>, set2_shape: Tensor<i64>, set_operation: String) {
        Self {
            id: new_id(),
            op_name: None,
            set1,
            set2_indices,
            set2_values,
            set2_shape,
            set_operation,
            validate_indices: true,
        }
    }
}

impl<T> Operation for DenseToSparseSetOperation<T>
where T: TensorType,
      T: con_or_DT_INT32_or_DT_UINT8_or_DT_INT16_or_DT_INT8_or_DT_STRING_or_DT_INT64_or_DT_UINT16,
{
    fn get_id(&self) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("DenseToSparseSetOperation", self.op_name)
        new_op.add_input(&self.set1)
        new_op.add_input(&self.set2_indices)
        new_op.add_input(&self.set2_values)
        new_op.add_input(&self.set2_shape)
        new_op.set_attr_string("set_operation", &set_operation)
        new_op.set_attr_bool("validate_indices", validate_indices)
        new_op.set_attr_type("T", T::data_type())
        new_op.finish()
    }
}

struct L2Loss<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_BFLOAT16_or_DT_HALF,
{
    id: usize,
    op_name: Option<String>,
    t: Tensor<T>,
}

impl<T> L2Loss<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_BFLOAT16_or_DT_HALF,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn output(&self) -> Tensor<T> {
        Tensor::<T>::new(self, 0)
    }

    fn new(t: Tensor<T>) {
        Self {
            id: new_id(),
            op_name: None,
            t,
        }
    }
}

impl<T> Operation for L2Loss<T>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_DOUBLE_or_DT_BFLOAT16_or_DT_HALF,
{
    fn get_id(&self) -> usize {
        self.id
    }

    fn tf_operation<'a>(&self, graph: &'a mut Graph) -> &'a TfOperation {
        if let Some(x) = graph.get_op_by_id(self.get_id()) {
            return x;
        }
        let new_op = graph.new_operation("L2Loss", self.op_name)
        new_op.add_input(&self.t)
        new_op.set_attr_type("T", T::data_type())
        new_op.finish()
    }
}

struct MaxPool3DGrad<T, TInput>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_BFLOAT16_or_DT_HALF,
      TInput: TensorType,
      TInput: con_or_DT_FLOAT_or_DT_BFLOAT16_or_DT_HALF,
{
    id: usize,
    op_name: Option<String>,
    orig_input: Tensor<TInput>,
    orig_output: Tensor<TInput>,
    grad: Tensor<T>,
    ksize: Vec<i64>,
    strides: Vec<i64>,
    padding: String,
    data_format: String,
}

impl<T, TInput> MaxPool3DGrad<T, TInput>
where T: TensorType,
      T: con_or_DT_FLOAT_or_DT_BFLOAT16_or_DT_HALF,
      TInput: TensorType,
      TInput: con_or_DT_FLOAT_or_DT_BFLOAT16_or_DT_HALF,
{
    fn op_name(&mut self, op_name: Option<String>) -> &mut Self {
        self.op_name = op_name;
        self
    }

    fn data_format(&mut self, data_format: String) -> &mut Self {
        self.data_format = data_format;
        self
    }

    fn output(&self) -> Tensor<T> {
        Tensor::<T>::new(self, 0)
    }

    fn new(orig_input: Tensor<TInput>, orig_output: Tensor<TInput>, grad: Tensor<T>, ksize: Vec<i64>, strides: Vec<i64>, padding: String) {
        Self {
            id: new_id(),
            op_name: None,
            orig_input,
            orig_output,
            grad,
            ksize,
            strides,
            padding,
            data_format: String::from_utf_lossy(&vec![78_u8, 68_u8, 72_u8, 87_u8, 67_u8,]),
        }
    }
}

impl<T, TInput> Operation for MaxPool3DGrad<T, TInput
